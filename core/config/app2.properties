############## input #################
input.folder=/huge1/people/chengli/projects/pyramid/archives/app1/ohsumed_20000/1
input.trainData=train
input.testData=test

############### output  ###########
output.folder=/huge1/people/chengli/projects/pyramid/archives/app2/ohsumed_20000/1
# the full name of the log file
# if not given, the log will be printed to the console
output.log=
############ function #############
train=true
tune=true
test=true

############# train ##########
train.warmStart=false
train.usePrior=true
train.numIterations=50
train.numLeaves=5
train.learningRate=0.1
train.minDataPerLeaf=3
train.numSplitIntervals=100
train.showTrainProgress=true
train.showTestProgress=true
train.showProgress.interval=5
# if earlyStop=true, the program will use test set performance (KL divergence) on each label to automatically decide when to stop training for that label
# make sure to have test set created before training starts and set train.showTestProgress=true
train.earlyStop=true
# the training will never stop before it reaches this minimum number of iterations
train.earlyStop.minIterations=20
# if the test performance for a label does not improve significantly after k successive evaluations, the training on that label will stop
# for example, if train.showProgress.interval=5 and train.earlyStop.patience=2, that means to stop if no significant improvement in 10 iterations
train.earlyStop.patience=2
# for an improvement to be significant, the absolute change must be bigger than this value
train.earlyStop.absoluteChange=0.1
# for an improvement to be significant, the relative change must be bigger than this value; 0.1 means 10%
train.earlyStop.relativeChange=0.1
train.generateReports=true

############ tune ###########
# after "train", run "tune" to search for the best threshold for the target evaluation measure
# tuning is necessary if predict.target=macroFMeasure

# whether to use train or test data to tune the threshold
tune.data=train

# The F-measure was derived so that F_\beta "measures the effectiveness of retrieval with respect to a user who attaches Î² times as much importance to recall as precision"
# Two other commonly used F measures are the F_{2} measure, which weights recall higher than precision, 
# and the F_{0.5} measure, which puts more emphasis on precision than recall. --wikipedia
tune.FMeasure.beta=1


############ predict ##############
# to achieve optimal prediction under which target measure 
# subsetAccuracy, hammingLoss, instanceFMeasure, macroFMeasure
# if prediction.target=macroFMeasure, user should run "tune" after "train"
predict.target=subsetAccuracy

############ report ##################
# the number of important features per class
report.topFeatures.limit=1000
report.rule.limit=10
report.numDocsPerFile=100
report.classProbThreshold=0.4
report.labelSetLimit=10
# whether to show prediction details in Json files
report.showPredictionDetail=true

# the internal Java class name for this application. 
# users do not need to modify this.
pyramid.class=App2
